{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4767abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "from dgl.nn.pytorch import edge_softmax\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch.nn.parameter import Parameter\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sig = nn.Sigmoid()\n",
    "hardtanh = nn.Hardtanh(0,1)\n",
    "gamma = -0.1\n",
    "zeta = 1.1\n",
    "beta = 0.66\n",
    "eps = 1e-20\n",
    "const1 = beta*np.log(-gamma/zeta + eps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493dab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l0_train(logAlpha, min, max):\n",
    "\t# draws reparameterizes Z for gradient estimation \n",
    "\t# according to the description of function f in Eq 12:\n",
    "\tU = torch.rand(logAlpha.size()).type_as(logAlpha) + eps\n",
    "\ts = sig((torch.log(U / (1 - U)) + logAlpha) / beta)\n",
    "\ts_bar = s * (zeta - gamma) + gamma\n",
    "\tmask = F.hardtanh(s_bar, min, max)\n",
    "\treturn mask\n",
    "\n",
    "def l0_test(logAlpha, min, max):\n",
    "\t# draws samples for Z (according to Eq 13) to be used in forward pass\n",
    "\ts = sig(logAlpha/beta)\n",
    "\ts_bar = s * (zeta - gamma) + gamma\n",
    "\tmask = F.hardtanh(s_bar, min, max)\n",
    "\treturn mask\n",
    "\n",
    "def get_loss2(logAlpha):\n",
    "\t\"\"\"\n",
    "\tSecond term in the right hand side of the loss fxn in Eq 12\n",
    "\t(except not yet scaled by lambda)\n",
    "\t\"\"\"\n",
    "\treturn sig(logAlpha - const1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60233d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseGraphAttentionLayer(nn.Module):\n",
    "\tdef __init__(self,\n",
    "\t\t\t\t graphs,\n",
    "\t\t\t\t in_dim,\n",
    "\t\t\t\t out_dim,\n",
    "\t\t\t\t num_heads,\n",
    "\t\t\t\t feat_drop,\n",
    "\t\t\t\t attn_drop,\n",
    "\t\t\t\t alpha,\n",
    "\t\t\t\t bias_l0,\n",
    "\t\t\t\t residual=False,l0=0, min=0):\n",
    "\t\tsuper(SparseGraphAttentionLayer, self).__init__()\n",
    "\t\tself.graphs = graphs\n",
    "\t\tself.num_graphs=len(self.graphs)\n",
    "\t\tself.num_heads = num_heads\n",
    "\t\tself.fc = nn.Linear(in_dim, num_heads * out_dim, bias=False)\n",
    "\t\tif feat_drop:\n",
    "\t\t\tself.feat_drop = nn.Dropout(feat_drop) #random feature dropout\n",
    "\t\telse:\n",
    "\t\t\tself.feat_drop = lambda x : x\n",
    "\t\tif attn_drop:\n",
    "\t\t\tself.attn_drop = nn.Dropout(attn_drop) #random attention (i.e. edge) dropout (in addition to the sparsity learned)\n",
    "\t\telse:\n",
    "\t\t\tself.attn_drop = lambda x : x\n",
    "\t\tself.attn_l = nn.Parameter(torch.Tensor(size=(1, 1, out_dim)))\n",
    "\t\tself.attn_r = nn.Parameter(torch.Tensor(size=(1, 1, out_dim)))\n",
    "\t\tself.bias_l0 = nn.Parameter(torch.FloatTensor([bias_l0]))\n",
    "\n",
    "\t\tnn.init.xavier_normal_(self.fc.weight.data, gain=1.414)\n",
    "\t\tnn.init.xavier_normal_(self.attn_l.data, gain=1.414)\n",
    "\t\tnn.init.xavier_normal_(self.attn_r.data, gain=1.414)\n",
    "\t\tself.leaky_relu = nn.LeakyReLU(alpha)\n",
    "\t\tself.softmax = edge_softmax\n",
    "\t\tself.residual = residual\n",
    "\t\tself.num = 0\n",
    "\t\tself.l0 = l0\n",
    "\t\tself.loss = 0\n",
    "\t\tself.dis = []\n",
    "\t\tself.min=min\n",
    "\t\tif residual:\n",
    "\t\t\tif in_dim != out_dim:\n",
    "\t\t\t\tself.res_fc = nn.Linear(in_dim, num_heads * out_dim, bias=False)\n",
    "\t\t\t\tnn.init.xavier_normal_(self.res_fc.weight.data, gain=1.414)\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.res_fc = None\n",
    "\n",
    "\tdef forward(self, inputs, edges=\"__ALL__\", skip=0):\n",
    "\t\tself.loss = 0\n",
    "\t\t# prepare\n",
    "\t\trets=[]\n",
    "\t\tfor g_i in tqdm(range(len(inputs))):\n",
    "\t\t\ttime.sleep(0.1) #for tqdm updates\n",
    "\t\t\th = self.feat_drop(inputs[g_i])  # NxD\n",
    "\t\t\th=inputs[g_i]\n",
    "\t\t\tft=self.fc(h)\n",
    "\t\t\tft = self.fc(h).reshape((h.shape[0], self.num_heads, -1))  # NxHxD'\n",
    "\t\t\ta1 = (ft * self.attn_l).sum(dim=-1).unsqueeze(-1) # N x H x 1\n",
    "\t\t\ta2 = (ft * self.attn_r).sum(dim=-1).unsqueeze(-1) # N x H x 1\n",
    "\t\t\tself.graphs[g_i].ndata.update({'ft' : ft, 'a1' : a1, 'a2' : a2})\n",
    "\n",
    "\t\t\tif skip == 0:\n",
    "\t\t\t\t# 1. compute edge attention\n",
    "\t\t\t\tself.graphs[g_i].apply_edges(self.edge_attention, edges)\n",
    "\n",
    "\t\t\t\t# 2. compute softmax\n",
    "\t\t\t\tif self.l0 == 1:\n",
    "\t\t\t\t\tind = self.graphs[g_i].nodes()\n",
    "\t\t\t\t\tself.graphs[g_i].apply_edges(self.loop, edges=(ind, ind))\n",
    "\n",
    "\t\t\t\tself.edge_softmax(g_i)\n",
    "\n",
    "\t\t\t\tif self.l0 == 1:\n",
    "\t\t\t\t\tself.graphs[g_i].apply_edges(self.norm)\n",
    "\n",
    "\t\t\t# 2. compute the aggregated node features scaled by the dropped,\n",
    "\t\t\t\tedges = self.graphs[g_i].edata['a'].squeeze().nonzero().squeeze()\n",
    "\n",
    "\n",
    "\t\t\tself.graphs[g_i].edata['a_drop'] = self.attn_drop(self.graphs[g_i].edata['a'])\n",
    "\t\t\tself.num = (self.graphs[g_i].edata['a'] > 0).sum()\n",
    "\t\t\tself.graphs[g_i].update_all(fn.src_mul_edge('ft', 'a_drop', 'ft'), fn.sum('ft', 'ft'))\n",
    "\t\t\tret = self.graphs[g_i].ndata['ft']\n",
    "\n",
    "\t\t\t# 4. residual\n",
    "\t\t\tif self.residual:\n",
    "\t\t\t\tif self.res_fc is not None:\n",
    "\t\t\t\t\tresval = self.res_fc(h).reshape((h.shape[0], self.num_heads, -1))  # NxHxD'\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tresval = torch.unsqueeze(h, 1)  # Nx1xD'\n",
    "\t\t\t\tret = resval + ret\n",
    "\t\t\tret = F.elu(ret.flatten(1))\n",
    "\t\t\trets.append(ret)\n",
    "\t\treturn rets, edges\n",
    "\t\t\t\n",
    "\tdef edge_attention(self, edges):\n",
    "\t\t# an edge UDF to compute unnormalized attention values from src and dst\n",
    "\t\tif self.l0 == 0:\n",
    "\t\t\tm = self.leaky_relu(edges.src['a1'] + edges.dst['a2'])\n",
    "\t\telse:\n",
    "\t\t\ttmp = edges.src['a1'] + edges.dst['a2']\n",
    "\t\t\tlogits = tmp + self.bias_l0\n",
    "\n",
    "\t\t\tif self.training:\n",
    "\t\t\t\tm = l0_train(logits, 0, 1)\n",
    "\t\t\telse:\n",
    "\t\t\t\tm = l0_test(logits, 0, 1)\n",
    "\t\t\tself.loss = get_loss2(logits[:,0,:]).sum()\n",
    "\t\treturn {'a': m}\n",
    "\n",
    "\tdef norm(self, edges):\n",
    "\t\t# normalize attention\n",
    "\t\ta = edges.data['a'] / edges.dst['z']\n",
    "\t\treturn {'a' : a}\n",
    "\n",
    "\tdef loop(self, edges):\n",
    "\t\t# set attention to itself as 1\n",
    "\t\treturn {'a': torch.pow(edges.data['a'], 0)}\n",
    "\t\t\n",
    "\tdef normalize(self, logits, g_i):\n",
    "\t\tself._logits_name = \"_logits\"\n",
    "\t\tself._normalizer_name = \"_norm\"\n",
    "\n",
    "\t\tself.graphs[g_i].edata[self._logits_name] = logits\n",
    "\t\tself.graphs[g_i].update_all(fn.copy_edge(self._logits_name, self._logits_name),\n",
    "\t\t\t\t\t\t fn.sum(self._logits_name, self._normalizer_name))\n",
    "\t\treturn self.graphs[g_i].edata.pop(self._logits_name), self.graphs[g_i].ndata.pop(self._normalizer_name)\n",
    "\n",
    "\tdef edge_softmax(self, g_i):\n",
    "\n",
    "\t\tif self.l0 == 0:\n",
    "\t\t\tscores = self.softmax(self.graphs[g_i], self.graphs[g_i].edata.pop('a'))\n",
    "\t\telse:\n",
    "\t\t\tscores, normalizer = self.normalize(self.graphs[g_i].edata.pop('a'), g_i)\n",
    "\t\t\tself.graphs[g_i].ndata['z'] = normalizer[:,0,:].unsqueeze(1)\n",
    "\n",
    "\t\tself.graphs[g_i].edata['a'] = scores[:,0,:].unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6286ac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGATDense(Linear):\n",
    "\n",
    "\tdef __init__(self, in_features, out_features, bias= True, device=None, dtype=None):\n",
    "\t\tfactory_kwargs = {'device': device, 'dtype': dtype}\n",
    "\t\tsuper(SGATDense, self).__init__(in_features, out_features)\n",
    "\n",
    "\t\tself.in_features = in_features\n",
    "\t\tself.out_features = out_features\n",
    "\t\tself.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n",
    "\t\tif bias:\n",
    "\t\t\tself.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n",
    "\t\telse:\n",
    "\t\t\tself.register_parameter('bias', None)\n",
    "\t\tself.reset_parameters()\n",
    "\n",
    "\tdef forward(self, inputs):\n",
    "\t\tlogits=torch.empty((len(inputs),4), dtype=torch.float32)\n",
    "\t\tfor i in tqdm(range(len(inputs))):\n",
    "\t\t\ttime.sleep(0.01) #for tqdm updates\n",
    "\t\t\tinp=inputs[i]\n",
    "\t\t\tlogit=F.linear(torch.transpose(inp,0,1), self.weight, self.bias)\n",
    "\t\t\t# prob=F.softmax(logit)\n",
    "\t\t\tlogits[i,:]=logit\n",
    "\t\t\t# logits.append(logit)\n",
    "\t\t\t# print(logit.dtype)\n",
    "\t\treturn logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c665efd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset):\n",
    "\tnum_samples = len(dataset)\n",
    "\tgraphs=[]\n",
    "\tlabels=[]\n",
    "\tinputs=[]\n",
    "\tfor i in range(num_samples):\n",
    "\t\tgraph,label=dataset[i]\n",
    "\t\tgraphs.append(graph),\n",
    "\t\tlabels.append(int(label['label']))\n",
    "\t\tinputs.append(graph.ndata[\"feat\"])\n",
    "\treturn graphs, labels, inputs\n",
    "\n",
    "def get_batch(dataset, batchSz):\n",
    "\tnum_samples = len(dataset)\n",
    "\tgraphs=[]\n",
    "\tlabels=[]\n",
    "\tinputs=[]\n",
    "\tidx=np.random.choice(num_samples, batchSz, replace=False)\n",
    "\tfor i in idx:\n",
    "\t\tgraph,label=dataset[i]\n",
    "\t\tgraphs.append(graph),\n",
    "\t\tlabels.append(int(label['label']))\n",
    "\t\tinputs.append(graph.ndata[\"feat\"])\n",
    "\treturn graphs, labels, inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430858c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGAT(nn.Module):\n",
    "\tdef __init__(self,\n",
    "\t\t\t\t graphs,\n",
    "\t\t\t\t num_layers,\n",
    "\t\t\t\t in_dim,\n",
    "\t\t\t\t num_hidden,\n",
    "\t\t\t\t num_classes,\n",
    "\t\t\t\t heads,\n",
    "\t\t\t\t alpha,\n",
    "\t\t\t\t feat_drop,\n",
    "\t\t\t\t attn_drop,\n",
    "\t\t\t\t bias_l0,\n",
    "\t\t\t\t residual, l0=0):\n",
    "\t\tsuper(SGAT, self).__init__()\n",
    "\t\tself.graphs = graphs\n",
    "\t\tself.num_layers = num_layers\n",
    "\t\tself.sgat_layers = nn.ModuleList()\n",
    "\t\t# input projection (no residual)\n",
    "\t\tself.sgat_layers.append(SparseGraphAttentionLayer(\n",
    "\t\t\tgraphs, in_dim, num_hidden, heads[0],feat_drop, attn_drop, alpha,bias_l0, False, l0=l0, min=0)) # \n",
    "\t\t# hidden layers\n",
    "\t\tfor l in range(1, num_layers):\n",
    "\t\t\t# due to multi-head, the in_dim = num_hidden * num_heads\n",
    "\t\t\tself.sgat_layers.append(SparseGraphAttentionLayer(\n",
    "\t\t\t\tgraphs, num_hidden * heads[l-1], num_hidden, heads[l], feat_drop, attn_drop,\n",
    "\t\t\t\t alpha,bias_l0, residual, l0=l0, min=0)) #feat_drop, attn_drop,\n",
    "\n",
    "\t\tself.denseFinal=SGATDense(num_hidden, num_classes)\n",
    "\n",
    "\t\tprint(\"Checking added layers\", self.sgat_layers)\n",
    "\n",
    "\n",
    "\tdef forward(self, inputs):\n",
    "\n",
    "\t\ths = inputs\n",
    "\t\tedges = \"__ALL__\"\n",
    "\t\ths, edges = self.sgat_layers[0](hs, edges)\n",
    "\t\t# hs = self.activation(h.flatten(1))\n",
    "\t\tfor l in range(1, self.num_layers):\n",
    "\t\t\ths, _= self.sgat_layers[l](hs, edges, skip=1)\n",
    "\t\tlogits =self.denseFinal(hs)\n",
    "\t\treturn logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb619bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "print(\"Loading in custom dataset\")\n",
    "dataset=dgl.data.CSVDataset('SNAREseq') #Once this is run, next time you can just import SNAREseq and do dataset=SNAREseq()\n",
    "graphs, labels, inputs=process_dataset(dataset)\n",
    "\n",
    "# num_train=int(len(dataset)*80/100)\t\n",
    "# num_val=int(len(dataset)*10/100)\n",
    "# num_test=int(len(dataset)*10/100)\n",
    "\n",
    "# train_idx=np.random.choice(len(dataset), num_train, replace=False)\n",
    "# val_idx=np.random.choice([i for i in range(len(dataset)) if i not in train_idx], num_val, replace=False)\n",
    "# test_idx=[i for i in range(len(dataset)) if i not in train_idx and i not in val_idx]\n",
    "# train_dataset=[dataset[i] for i in train_idx]\n",
    "\n",
    "\n",
    "#Set hyperparameters (need to calibrate these)\n",
    "num_heads=1\n",
    "num_layers=1\n",
    "num_out_heads=1\n",
    "heads = ([num_heads] * num_layers) + [num_out_heads]\n",
    "num_hidden=2000\n",
    "alpha=0.2\n",
    "bias_l0=0\n",
    "l0=0\n",
    "residual=False\n",
    "feat_drop=None\n",
    "attn_drop=None \n",
    "#Fixed parameters:\n",
    "num_classes=max(labels)+1\n",
    "in_dim=len(inputs[0])\n",
    "\n",
    "model=SGAT(graphs,num_layers=num_layers,\n",
    "\t\t\t\t in_dim=in_dim,\n",
    "\t\t\t\t num_hidden=num_hidden,\n",
    "\t\t\t\t num_classes=num_classes,\n",
    "\t\t\t\t heads=heads,\n",
    "\t\t\t\t alpha=alpha,\n",
    "\t\t\t\t feat_drop=feat_drop,\n",
    "\t\t\t\t attn_drop=attn_drop,\n",
    "\t\t\t\t bias_l0=bias_l0,\n",
    "\t\t\t\t residual=residual, l0=l0)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "labels=torch.LongTensor(labels)\n",
    "\n",
    "def train():\n",
    "\tmodel.train()\n",
    "\tout = model(inputs)  # Perform a single forward pass.\n",
    "\tloss = criterion(out, labels)  # Compute the loss.\n",
    "\ttry:\n",
    "\t\tloss.backward()  \n",
    "\t\toptimizer.step() \n",
    "\t\toptimizer.zero_grad() \n",
    "\texcept:\n",
    "\t\tpass\n",
    "\n",
    "def test():\n",
    "\tmodel.eval()\n",
    "\n",
    "\tcorrect = 0\n",
    "\ttest_graphs,test_labels,test_inputs=get_batch(dataset, 100)\n",
    "\ttest_labels=torch.LongTensor(test_labels)\n",
    "\tout = model(test_inputs) \n",
    "\tpred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "\tcorrect += int((pred == test_labels).sum())  # Check against ground-truth labels.\n",
    "\treturn correct / len(test_labels)  # Derive ratio of correct predictions.\n",
    "\n",
    "for i in range(100):\n",
    "\tprint(\"EPOCH\", i)\n",
    "\ttrain()\n",
    "\tprint(test())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
